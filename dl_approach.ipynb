{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "010da5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2b7197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Spectrograms: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1744/1744 [00:40<00:00, 42.92it/s]\n"
     ]
    }
   ],
   "source": [
    "np.complex = complex  # ðŸ”§ Patch for librosa compatibility\n",
    "# === CONFIG ===\n",
    "AUDIO_DIR = r\"D:\\DEAM_audio_wav\"       # <-- your .wav folder with 1744 files\n",
    "OUTPUT_DIR = r\"C:\\Users\\Ashutosh Gupta\\OneDrive\\ë¬¸ì„œ\\emotion_recognition_research\\data\\NPY_files\"     # <-- where you want to save .npy\n",
    "SAMPLE_RATE = 22050\n",
    "DURATION = 3  # seconds\n",
    "N_MELS = 128\n",
    "HOP_LENGTH = 512\n",
    "FIXED_SHAPE = (128, 128)\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# === Function to extract fixed-size log-mel spectrogram ===\n",
    "def extract_logmel(file_path, sr=SAMPLE_RATE, duration=DURATION):\n",
    "    y, _ = librosa.load(file_path, sr=sr, duration=duration)\n",
    "\n",
    "    # Pad or trim\n",
    "    desired_len = sr * duration\n",
    "    if len(y) < desired_len:\n",
    "        y = np.pad(y, (0, desired_len - len(y)))\n",
    "    else:\n",
    "        y = y[:desired_len]\n",
    "\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=N_MELS, hop_length=HOP_LENGTH)\n",
    "    log_mel = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "    # Resize to fixed shape\n",
    "    if log_mel.shape[1] < FIXED_SHAPE[1]:\n",
    "        pad_width = FIXED_SHAPE[1] - log_mel.shape[1]\n",
    "        log_mel = np.pad(log_mel, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        log_mel = log_mel[:, :FIXED_SHAPE[1]]\n",
    "\n",
    "    return log_mel\n",
    "\n",
    "# === Batch Process and Save ===\n",
    "all_files = sorted([f for f in os.listdir(AUDIO_DIR) if f.endswith('.wav')])\n",
    "\n",
    "for file in tqdm(all_files, desc=\"Extracting Spectrograms\"):\n",
    "    file_path = os.path.join(AUDIO_DIR, file)\n",
    "    song_id = os.path.splitext(file)[0]\n",
    "    try:\n",
    "        mel = extract_logmel(file_path)\n",
    "        np.save(os.path.join(OUTPUT_DIR, f\"{song_id}.npy\"), mel)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing {file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d1dc7164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# === CONFIG ===\n",
    "NPY_DIR = r\"C:\\Users\\Ashutosh Gupta\\OneDrive\\ë¬¸ì„œ\\emotion_recognition_research\\data\\NPY_files\"\n",
    "LABEL_CSV =r'labels_va.csv'\n",
    "\n",
    "# === Load labels (valence/arousal mean) ===\n",
    "label_df = pd.read_csv(LABEL_CSV)\n",
    "label_df = label_df[['song_id', ' valence_mean', ' arousal_mean']]\n",
    "label_df['song_id'] = label_df['song_id'].astype(str)\n",
    "label_map = label_df.set_index('song_id').to_dict('index')\n",
    "\n",
    "# === Dataset Class ===\n",
    "class LogMelEmotionDataset(Dataset):\n",
    "    def __init__(self, npy_dir, label_map, target='valence_mean'):\n",
    "        self.npy_dir = npy_dir\n",
    "        self.label_map = label_map\n",
    "        self.target = target\n",
    "        self.file_list = [f for f in os.listdir(npy_dir) if f.endswith('.npy')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.file_list[idx]\n",
    "        song_id = os.path.splitext(file)[0]\n",
    "\n",
    "        # Load spectrogram\n",
    "        mel = np.load(os.path.join(self.npy_dir, file))\n",
    "        mel_tensor = torch.tensor(mel, dtype=torch.float32).unsqueeze(0)  # (1, 128, 128)\n",
    "\n",
    "        # Get label\n",
    "        label = self.label_map[song_id][self.target]\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        return mel_tensor, label_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1dc7abc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# === Hyperparams ===\n",
    "BATCH_SIZE = 32\n",
    "VAL_SPLIT = 0.2\n",
    "SEED = 42\n",
    "\n",
    "# === Full Dataset for Valence or Arousal ===\n",
    "dataset = LogMelEmotionDataset(NPY_DIR, label_map, target=' valence_mean')  # or 'arousal_mean'\n",
    "\n",
    "# === Split ===\n",
    "val_len = int(len(dataset) * VAL_SPLIT)\n",
    "train_len = len(dataset) - val_len\n",
    "\n",
    "train_set, val_set = random_split(dataset, [train_len, val_len], generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "# === Dataloaders ===\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e0c19704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CNN Model Definition ===\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EmotionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmotionCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)  # output: valence or arousal\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x.squeeze(1)  # (batch,) for regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "463b1d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Squeeze-and-Excitation Block ---\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.global_pool(x).view(b, c)\n",
    "        y = F.relu(self.fc1(y))\n",
    "        y = torch.sigmoid(self.fc2(y)).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "# --- ADFF-Inspired CNN Model ---\n",
    "class EmotionCNN_SE_Temporal(nn.Module):\n",
    "    def __init__(self, input_shape=(1, 128, 128)):\n",
    "        super(EmotionCNN_SE_Temporal, self).__init__()\n",
    "\n",
    "        # --- Convolutional blocks with SE Attention ---\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.se1 = SEBlock(16)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.se2 = SEBlock(32)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.se3 = SEBlock(64)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # --- Temporal modeling ---\n",
    "        self.temporal_conv = nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # --- Infer flattened size ---\n",
    "        dummy = torch.zeros(1, *input_shape)\n",
    "        with torch.no_grad():\n",
    "            x = self.forward_features(dummy)\n",
    "            self.flattened_size = x.shape[1]\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.se1(x)\n",
    "\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.se2(x)\n",
    "\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.se3(x)\n",
    "\n",
    "        b, c, h, w = x.shape\n",
    "        x = x.view(b, c, -1)           # (B, C, Time*Freq)\n",
    "        x = self.temporal_conv(x)      # Conv1D\n",
    "        x = F.relu(x)\n",
    "        x = x.view(b, -1)              # Flatten\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0de557e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.shape\n",
    "        y = self.pool(x).view(b, c)\n",
    "        y = F.relu(self.fc1(y))\n",
    "        y = torch.sigmoid(self.fc2(y)).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "class EmotionCNN_GRU(nn.Module):\n",
    "    def __init__(self, input_shape=(1, 128, 128), hidden_size=64, num_layers=1):\n",
    "        super(EmotionCNN_GRU, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.se1 = SEBlock(16)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.se2 = SEBlock(32)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.se3 = SEBlock(64)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Determine GRU input size\n",
    "        dummy = torch.zeros(1, *input_shape)\n",
    "        with torch.no_grad():\n",
    "            x = self.forward_conv(dummy)\n",
    "            _, c, t, f = x.shape\n",
    "            self.rnn_input_size = f * c\n",
    "            self.seq_len = t\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.rnn_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(hidden_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward_conv(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.se1(x)\n",
    "\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.se2(x)\n",
    "\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.se3(x)\n",
    "\n",
    "        return x  # shape: (B, C, T, F)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_conv(x)  # (B, C, T, F)\n",
    "        b, c, t, f = x.shape\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  # (B, T, C, F)\n",
    "        x = x.view(b, t, -1)  # Flatten last two dims â†’ (B, T, C*F)\n",
    "\n",
    "        _, h_n = self.gru(x)  # Only take final hidden state (h_n)\n",
    "        x = h_n[-1]  # shape: (B, hidden_size)\n",
    "\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "70bf0dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Squeeze-and-Excitation Block ---\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.shape\n",
    "        y = self.pool(x).view(b, c)\n",
    "        y = F.relu(self.fc1(y))\n",
    "        y = torch.sigmoid(self.fc2(y)).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "# --- GRU + Attention Module ---\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, gru_output):\n",
    "        # gru_output: (B, T, H)\n",
    "        attn_weights = F.softmax(self.attn(gru_output), dim=1)  # (B, T, 1)\n",
    "        context = torch.sum(attn_weights * gru_output, dim=1)   # (B, H)\n",
    "        return context, attn_weights\n",
    "\n",
    "# --- Final Model ---\n",
    "class EmotionCNN_GRU_Attention(nn.Module):\n",
    "    def __init__(self, input_shape=(1, 128, 128), hidden_size=64, num_layers=1):\n",
    "        super(EmotionCNN_GRU_Attention, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.se1 = SEBlock(16)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.se2 = SEBlock(32)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.se3 = SEBlock(64)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Determine GRU input size\n",
    "        dummy = torch.zeros(1, *input_shape)\n",
    "        with torch.no_grad():\n",
    "            x = self.forward_conv(dummy)\n",
    "            _, c, t, f = x.shape\n",
    "            self.rnn_input_size = f * c\n",
    "            self.seq_len = t\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.rnn_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.attention = Attention(hidden_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(hidden_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward_conv(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.se1(x)\n",
    "\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.se2(x)\n",
    "\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.se3(x)\n",
    "\n",
    "        return x  # (B, C, T, F)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_conv(x)  # (B, C, T, F)\n",
    "        b, c, t, f = x.shape\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  # (B, T, C, F)\n",
    "        x = x.view(b, t, -1)  # (B, T, C*F)\n",
    "\n",
    "        gru_out, _ = self.gru(x)  # (B, T, H)\n",
    "        context, attn_weights = self.attention(gru_out)  # (B, H)\n",
    "\n",
    "        x = self.dropout(F.relu(self.fc1(context)))\n",
    "        x = self.fc2(x)\n",
    "        return x.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b1623d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ashutosh Gupta\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Loss: 4.7136 | R2: 0.1311 | RMSE: 1.0622 | MAE: 0.8720\n",
      "Epoch 2/50 - Loss: 1.2374 | R2: -0.1100 | RMSE: 1.2005 | MAE: 0.9898\n",
      "Epoch 3/50 - Loss: 1.1158 | R2: 0.0340 | RMSE: 1.1199 | MAE: 0.8997\n",
      "Epoch 4/50 - Loss: 1.0556 | R2: 0.4231 | RMSE: 0.8654 | MAE: 0.6784\n",
      "Epoch 5/50 - Loss: 1.0603 | R2: 0.3326 | RMSE: 0.9309 | MAE: 0.7253\n",
      "Epoch 6/50 - Loss: 1.0106 | R2: 0.4155 | RMSE: 0.8711 | MAE: 0.6832\n",
      "Epoch 7/50 - Loss: 0.9426 | R2: 0.3369 | RMSE: 0.9278 | MAE: 0.7337\n",
      "Epoch 8/50 - Loss: 0.9363 | R2: 0.4381 | RMSE: 0.8541 | MAE: 0.6733\n",
      "Epoch 9/50 - Loss: 0.8865 | R2: 0.3623 | RMSE: 0.9099 | MAE: 0.7051\n",
      "Epoch 10/50 - Loss: 0.8319 | R2: 0.4189 | RMSE: 0.8686 | MAE: 0.6738\n",
      "Epoch 11/50 - Loss: 0.8744 | R2: 0.2742 | RMSE: 0.9708 | MAE: 0.7652\n",
      "Epoch 12/50 - Loss: 0.8331 | R2: 0.4187 | RMSE: 0.8688 | MAE: 0.6803\n",
      "Epoch 13/50 - Loss: 0.7718 | R2: 0.3100 | RMSE: 0.9465 | MAE: 0.7418\n",
      "Epoch 14/50 - Loss: 0.7816 | R2: 0.4650 | RMSE: 0.8334 | MAE: 0.6496\n",
      "Epoch 15/50 - Loss: 0.7428 | R2: -0.0822 | RMSE: 1.1854 | MAE: 0.9531\n",
      "Epoch 16/50 - Loss: 0.7704 | R2: 0.4348 | RMSE: 0.8567 | MAE: 0.6717\n",
      "Epoch 17/50 - Loss: 0.6996 | R2: 0.3981 | RMSE: 0.8840 | MAE: 0.6906\n",
      "Epoch 18/50 - Loss: 0.6577 | R2: 0.3637 | RMSE: 0.9089 | MAE: 0.7214\n",
      "Epoch 19/50 - Loss: 0.6519 | R2: 0.3872 | RMSE: 0.8920 | MAE: 0.7082\n",
      "Epoch 20/50 - Loss: 0.6170 | R2: 0.3676 | RMSE: 0.9062 | MAE: 0.7223\n",
      "Epoch 21/50 - Loss: 0.5539 | R2: 0.2701 | RMSE: 0.9735 | MAE: 0.7751\n",
      "Epoch 22/50 - Loss: 0.5064 | R2: 0.1861 | RMSE: 1.0280 | MAE: 0.8236\n",
      "Epoch 23/50 - Loss: 0.4734 | R2: 0.3722 | RMSE: 0.9028 | MAE: 0.7134\n",
      "Epoch 24/50 - Loss: 0.4189 | R2: 0.2822 | RMSE: 0.9654 | MAE: 0.7772\n",
      "ðŸ›‘ Early stopping at epoch 24. Best R2: 0.4650\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EmotionCNN_GRU_Attention().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "EPOCHS = 50\n",
    "early_stop_patience = 10\n",
    "best_r2 = -np.inf\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # ðŸ”§ Gradient clipping (useful for RNNs)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            preds.extend(outputs.cpu().numpy())\n",
    "            trues.extend(targets.cpu().numpy())\n",
    "\n",
    "    r2 = r2_score(trues, preds)\n",
    "    mse = mean_squared_error(trues, preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(trues, preds)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {avg_loss:.4f} | R2: {r2:.4f} | RMSE: {rmse:.4f} | MAE: {mae:.4f}\")\n",
    "\n",
    "    # ðŸ”§ Learning rate scheduler\n",
    "    scheduler.step(r2)\n",
    "\n",
    "    # ðŸ”’ Early Stopping\n",
    "    if r2 > best_r2:\n",
    "        best_r2 = r2\n",
    "        best_model_state = model.state_dict()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stop_patience:\n",
    "            print(f\"ðŸ›‘ Early stopping at epoch {epoch+1}. Best R2: {best_r2:.4f}\")\n",
    "            break\n",
    "\n",
    "# âœ… Load best model state (optional)\n",
    "if best_model_state:\n",
    "    model.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f620a19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
